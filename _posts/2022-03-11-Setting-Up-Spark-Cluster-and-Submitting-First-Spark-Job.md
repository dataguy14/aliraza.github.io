---
title: "Setting Up Spark Cluster and Submitting Your First Spark Job"
date:   2022-03-11
permalink: /Setting-Up-Spark-Cluster-and-Submitting-First-Spark-Job/
tags:
  - AWS
  - Amazon EMR
  - Apache Spark
  - PySpark
---

The article provides an overview of Apache Spark and its components, as well as instructions on setting up a Spark cluster on a local machine. It explains that Apache Spark is a distributed processing system used for handling Big Data workloads. The architecture consists of three main components: the Driver Program, Cluster Manager, and Worker Node(s). The article also introduces four libraries in Apache Spark: Spark SQL, Spark Streaming, Spark MLlib, and Spark GraphX, explaining their respective functionalities. The instructions then guide the reader through the process of installing dependencies, downloading and setting up Spark binaries, configuring environment variables, starting the Spark Driver Program and Worker Node, and running code on the Spark local cluster. The author concludes by sharing their personal experience and inviting constructive criticism from the audience.

Article link : [Here](https://medium.com/red-buffer/setting-up-spark-cluster-and-submitting-your-first-spark-job-13410e7ac71f)
